{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8445eec4-1a10-4070-9757-c0696b4f94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, protocol, data, labels, type, subject_id_list=None, num_freq=None):\n",
    "        uni = np.unique(subject_id_list)\n",
    "        # np.unique() 提取 subject_id_list 中唯一的被试ID\n",
    "        if protocol == 'cross_subject':\n",
    "            dict = {}\n",
    "            for i in range(len(uni)):\n",
    "                dict.update({uni[i]: i})\n",
    "            sum = [0 for i in range(len(uni))]\n",
    "            for i in range(len(subject_id_list)):\n",
    "                sum[dict[subject_id_list[i]]] += 1\n",
    "            # \n",
    "            for i in range(1, len(uni)):\n",
    "                sum[i] += sum[i-1]\n",
    "            data_new = np.zeros_like(data)\n",
    "            labels_new = np.zeros_like(labels)\n",
    "            subject_id_list_new = np.zeros_like(subject_id_list)\n",
    "            pre_sum = copy.deepcopy(sum)\n",
    "            self.pre_sum = pre_sum\n",
    "            for i in range(data.shape[0]):\n",
    "                pos = sum[dict[subject_id_list[i]]]-1\n",
    "                sum[dict[subject_id_list[i]]] -= 1\n",
    "                data_new[pos, :, :] = data[i, :, :]\n",
    "                labels_new[pos] = labels[i]\n",
    "                subject_id_list_new[pos] = subject_id_list[i]\n",
    "\n",
    "        else:  # intra_subject. A random shuffle is used to reorder the data.\n",
    "            lst = [i for i in range(data.shape[0])]\n",
    "            random.shuffle(lst)\n",
    "            data_new = np.zeros_like(data)\n",
    "            labels_new = np.zeros_like(labels)\n",
    "            subject_id_list_new = np.zeros_like(subject_id_list)\n",
    "            for i in range(data.shape[0]):\n",
    "                pos = lst[i]\n",
    "                data_new[pos, :, :] = data[i, :, :]\n",
    "                labels_new[pos] = labels[i]\n",
    "                subject_id_list_new[pos] = subject_id_list[i]\n",
    "\n",
    "        self.type = type\n",
    "        self.protocol = protocol\n",
    "        self.data = data_new\n",
    "        self.labels = labels_new\n",
    "        self.num_freq = num_freq\n",
    "        self.subject_id_list = subject_id_list_new\n",
    "\n",
    "    def getData(self, K, fold):\n",
    "        if self.protocol == 'cross_subject':\n",
    "            uni = np.unique(self.subject_id_list)\n",
    "            subject_num = len(uni)\n",
    "            if K > subject_num:\n",
    "                raise ValueError(\n",
    "                    f\"Under cross_subject protocol, there are {subject_num} subjects in the dataset, which is smaller than the fold number {K}. So, the dataset can not be divided into {K} sections.\")\n",
    "\n",
    "            step = int(subject_num/K)\n",
    "            st = step*fold\n",
    "            nd = step*(fold+1)  # [st,nd)\n",
    "            if fold == K-1:\n",
    "                nd = subject_num\n",
    "            left = 0\n",
    "            right = len(self.subject_id_list)\n",
    "            if st != 0:\n",
    "                left = self.pre_sum[st-1]\n",
    "            if nd != subject_num:\n",
    "                right = self.pre_sum[nd-1]\n",
    "\n",
    "        else:\n",
    "            tot_num = len(self.subject_id_list)\n",
    "            if tot_num < K:\n",
    "                raise ValueError(\n",
    "                    f\"Under intra_subject protocol, there are {tot_num} samples in the dataset, which is smaller than the fold number {K}. So, the dataset can not be divided into {K} sections.\")\n",
    "\n",
    "            step = int(tot_num/K)\n",
    "            st = step*fold\n",
    "            nd = step*(fold+1)  # [st,nd)\n",
    "            if fold == K-1:\n",
    "                nd = tot_num\n",
    "            left = st\n",
    "            right = nd\n",
    "\n",
    "        valid_list = np.arange(left, right)\n",
    "        train_list = np.array(\n",
    "            list(set(np.arange(0, len(self.subject_id_list))) - set(valid_list))).astype(int)\n",
    "        data_train = self.data[train_list, :, :]\n",
    "        label_train = self.labels[train_list]\n",
    "        data_val = self.data[valid_list, :, :]\n",
    "        label_val = self.labels[valid_list]\n",
    "        train_subject_list = self.subject_id_list[train_list]\n",
    "        return data_train, label_train, data_val, label_val, train_subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2705ba8-18aa-4a93-8fe7-81214dabf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(protocol, data, labels, subject_id_list=None, data_time=None):\n",
    "    if protocol != \"cross_subject\" and protocol != \"intra_subject\":\n",
    "        raise ValueError(\n",
    "            \"The protocol should be either 'cross_subject' or 'intra_subject'.\")\n",
    "    if protocol == \"cross_subject\" and subject_id_list is None:\n",
    "        raise ValueError(\n",
    "            \"The subject_id_list should not be none when data splitting protocol is cross_subject\")\n",
    "\n",
    "    num_freq = data.shape[-1]\n",
    "    if data_time is not None:\n",
    "        data = np.concatenate((data, data_time), axis=2)\n",
    "\n",
    "    dataloader = DataLoader(protocol, data, labels,\n",
    "                            'user_defined', subject_id_list=subject_id_list, num_freq=num_freq)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2672aa-05eb-482b-9ff1-c7d244018c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_FACED(protocol, categories, data_path):\n",
    "    if protocol != \"cross_subject\" and protocol != \"intra_subject\":\n",
    "        raise ValueError(\n",
    "            \"The protocol should be either 'cross_subject' or 'intra_subject'.\")\n",
    "    if categories != 2 and categories != 9:\n",
    "        raise ValueError(\"The categories should be either 2 or 9.\")\n",
    "    if os.path.exists(data_path) == False:\n",
    "        raise ValueError(\"The path of FACED dataset does not exist.\")\n",
    "\n",
    "    if categories != 2 and categories != 9:\n",
    "        raise ValueError(\n",
    "            \"The label categories in FACED dataset should be either 2 or 9.\")\n",
    "\n",
    "    # Note: the signals are in dict['de_lds']\n",
    "    data = hdf5.loadmat(data_path)['de_lds']\n",
    "    # data shape: (123, 720 or 840, 120)  120=4*30  4 band and 30 nodes\n",
    "    label_type = 'cls2' if categories == 2 else 'cls9'\n",
    "    data, label_repeat, n_samples = load_srt_de(\n",
    "        data, True, False, 1, label_type)\n",
    "    # label_repeat shape: 720 or 840   720=24*30. 840=28*30   24/28 videos and 30 samples are generated by each video\n",
    "\n",
    "    feature_shape = int(data.shape[-1]/30)\n",
    "    labels = np.tile(label_repeat, data.shape[0])\n",
    "    data = data.reshape(-1, feature_shape, 30).transpose([0, 2, 1])\n",
    "\n",
    "    subject_id_list = [int(i/len(label_repeat))\n",
    "                       for i in range(labels.shape[0])]\n",
    "    dataloader = DataLoader(protocol, data, labels,\n",
    "                            'FACED', subject_id_list=subject_id_list)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51af0380-c76b-4184-87f4-cebb514a2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGridPara(paras):\n",
    "    if \"dropout\" in paras:\n",
    "        dropout = paras[\"dropout\"]\n",
    "    else:\n",
    "        dropout = 0.5\n",
    "    if \"batch_size\" in paras:\n",
    "        batch_size = paras[\"batch_size\"]\n",
    "    else:\n",
    "        batch_size = 256\n",
    "    if \"lr\" in paras:\n",
    "        lr = paras[\"lr\"]\n",
    "    else:\n",
    "        lr = 5e-3\n",
    "    if \"l1_reg\" in paras:\n",
    "        l1_reg = paras[\"l1_reg\"]\n",
    "    else:\n",
    "        l1_reg = 0\n",
    "    if \"l2_reg\" in paras:\n",
    "        l2_reg = paras[\"l2_reg\"]\n",
    "    else:\n",
    "        l2_reg = 0\n",
    "    return lr, batch_size, dropout, l1_reg, l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e664ea77-992c-41f5-9bab-1d83e5dccbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch(curModel, data_train, label_train, data_val, label_val,\n",
    "           device, optimizer, categories, dropout, batch_size, lr, l1_reg, l2_reg,\n",
    "           mx_epoch, num_freq=None,\n",
    "           NodeDAT=False, EmotionDL=False, train_log=False\n",
    "           ):\n",
    "    if isinstance(curModel, RGNN):\n",
    "        curModel.train_and_eval(\n",
    "            data_train, label_train, data_val, label_val,\n",
    "            device, optimizer, categories, dropout,\n",
    "            batch_size=batch_size, lr=lr, l1_reg=l1_reg, l2_reg=l2_reg,\n",
    "            NodeDAT=NodeDAT, EmotionDL=EmotionDL,\n",
    "            num_epoch=mx_epoch, train_log=train_log\n",
    "        )\n",
    "    elif isinstance(curModel, HetEmotionNet):\n",
    "        curModel.train_and_eval(\n",
    "            data_train, label_train, data_val, label_val, num_freq,\n",
    "            device, optimizer, categories, dropout,\n",
    "            batch_size=batch_size, lr=lr, l1_reg=l1_reg, l2_reg=l2_reg,\n",
    "            num_epoch=mx_epoch, train_log=train_log\n",
    "        )\n",
    "    else:\n",
    "        curModel.train_and_eval(\n",
    "            data_train, label_train, data_val, label_val,\n",
    "            device, optimizer, categories, dropout,\n",
    "            batch_size=batch_size, lr=lr, l1_reg=l1_reg, l2_reg=l2_reg,\n",
    "            num_epoch=mx_epoch, train_log=train_log\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4645f9-06cf-46e1-8095-3c4c38340e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model: GNNModel, loader: DataLoader, protocol: str, grid: dict, categories, K, K_inner=None, device=torch.device('cpu'),\n",
    "               optimizer=\"Adam\", NodeDAT=False, train_log=False):\n",
    "    EmotionDL = False\n",
    "    # ,L1_reg=0,L2_reg=0,dropout=0,alpha=0,lr,epoch=100,batch_size,\n",
    "    if protocol != \"cv\" and protocol != \"ncv\" and protocol != \"fcv\":\n",
    "        raise ValueError(\n",
    "            \"The evaluation protocols must be 'cv', 'fcv' or 'ncv'.\")\n",
    "    model_paras = [\"hiddens\", \"layers\"]\n",
    "    train_paras = [\"lr\", \"epoch\", \"dropout\", \"batch_size\", \"l1_reg\", \"l2_reg\"]\n",
    "    grid_paras = []\n",
    "    grid_epoch = [50]\n",
    "\n",
    "    for k, v in grid.items():\n",
    "        if k not in model_paras and k not in train_paras:\n",
    "            raise ValueError(\n",
    "                f\"The parameter name {k} does not exist or can not be tuned.\")\n",
    "        if type(v) is not list and isinstance(v, (int, float)) == False:\n",
    "            raise ValueError(\n",
    "                f\"The type of parameter value {v} must be list, int or float.\")\n",
    "\n",
    "        if k == \"epoch\":\n",
    "            if type(v) is list:\n",
    "                grid_epoch = v\n",
    "            else:\n",
    "                grid_epoch = [v]\n",
    "            continue\n",
    "\n",
    "        if len(grid_paras) == 0:\n",
    "            if type(v) is list:\n",
    "                for _ in v:\n",
    "                    grid_paras.append({k: _})\n",
    "            else:\n",
    "                grid_paras.append({k: v})\n",
    "        else:\n",
    "            tmp = []\n",
    "            if type(v) is list:\n",
    "                for _ in v:\n",
    "                    nt = {k: _}\n",
    "                    for old in grid_paras:\n",
    "                        ndict = copy.deepcopy(old)\n",
    "                        ndict.update(nt)\n",
    "                        tmp.append(ndict)\n",
    "            else:\n",
    "                nt = {k: v}\n",
    "                for old in grid_paras:\n",
    "                    ndict = copy.deepcopy(old)\n",
    "                    ndict.update(nt)\n",
    "                    tmp.append(ndict)\n",
    "            grid_paras = tmp\n",
    "\n",
    "    mx_epoch = max(grid_epoch)+1\n",
    "    if protocol == \"cv\" or protocol == \"fcv\":\n",
    "        result_list = []\n",
    "        for paras in grid_paras:\n",
    "            nModel = copy.deepcopy(model)\n",
    "            if \"hiddens\" in paras:\n",
    "                nModel.num_hiddens = paras[\"hiddens\"]\n",
    "            if \"layers\" in paras:\n",
    "                nModel.num_layers = paras[\"layers\"]\n",
    "\n",
    "            acc_list = []\n",
    "            lr, batch_size, dropout, l1_reg, l2_reg = getGridPara(paras)\n",
    "            for fold in range(K):\n",
    "                data_train, label_train, data_val, label_val, train_subject_list = loader.getData(\n",
    "                    K, fold)\n",
    "                curModel = copy.deepcopy(nModel)\n",
    "                launch(curModel, data_train, label_train, data_val, label_val, device, optimizer, categories, dropout,\n",
    "                       batch_size, lr, l1_reg, l2_reg, mx_epoch,\n",
    "                       loader.num_freq, NodeDAT, EmotionDL, train_log)\n",
    "                acc_list.append(\n",
    "                    (curModel.trainer.eval_acc_list, data_val.shape[0]))\n",
    "\n",
    "            # calc the acc result under the given paras\n",
    "\n",
    "            acc_result = 0\n",
    "            tot_samples = 0\n",
    "            argmax_epoch = -1\n",
    "            if protocol == 'cv':\n",
    "                for fold in range(K):\n",
    "                    max_acc = 0\n",
    "                    for ep in grid_epoch:\n",
    "                        max_acc = max(max_acc, acc_list[fold][0][ep])\n",
    "                    acc_result += max_acc*acc_list[fold][1]\n",
    "                    tot_samples += acc_list[fold][1]\n",
    "                assert (tot_samples == loader.data.shape[0])\n",
    "                acc_result /= tot_samples\n",
    "            else:  # fcv\n",
    "                acc_ep = []\n",
    "                for ep in grid_epoch:\n",
    "                    acc_ep.append(0)\n",
    "                    tot_samples = 0\n",
    "                    for fold in range(K):\n",
    "                        acc_ep[-1] += acc_list[fold][0][ep]*acc_list[fold][1]\n",
    "                        tot_samples += acc_list[fold][1]\n",
    "                    assert (tot_samples == loader.data.shape[0])\n",
    "                    acc_ep[-1] /= tot_samples\n",
    "                acc_result = max(acc_ep)\n",
    "                argmax_epoch = np.argmax(np.array(acc_ep))+1\n",
    "            result_list.append(\n",
    "                {\"paras\": paras, \"acc_mean\": acc_result, \"argmax_epoch\": argmax_epoch})\n",
    "        best_dict = result_list[0]\n",
    "        for i in range(1, len(result_list)):\n",
    "            if result_list[i][\"acc_mean\"] > best_dict[\"acc_mean\"]:\n",
    "                best_dict = result_list[i]\n",
    "        return copy.deepcopy(best_dict), result_list\n",
    "\n",
    "    else:  # ncv\n",
    "        if K_inner is None:\n",
    "            raise ValueError(\n",
    "                \"K_inner must not be None when the protocol is set as 'ncv'.\")\n",
    "\n",
    "        out_acc_list = []\n",
    "        for out_fold in range(K):\n",
    "            data_train_and_val, label_train_and_val, data_test, label_test, train_and_val_subject_list = loader.getData(\n",
    "                K, out_fold)\n",
    "            # print(\"out: \",out_fold,\" : \",label_train_and_val.shape,label_test.shape)\n",
    "            result_list = []\n",
    "            for paras in grid_paras:\n",
    "                nModel = copy.deepcopy(model)\n",
    "                if \"hiddens\" in paras:\n",
    "                    nModel.num_hiddens = paras[\"hiddens\"]\n",
    "                if \"layers\" in paras:\n",
    "                    nModel.num_layers = paras[\"layers\"]\n",
    "\n",
    "                acc_list = []\n",
    "                lr, batch_size, dropout, l1_reg, l2_reg = getGridPara(paras)\n",
    "\n",
    "                for in_fold in range(K_inner):\n",
    "                    nLoader = DataLoader(loader.protocol, data_train_and_val, label_train_and_val,\n",
    "                                         loader.type, subject_id_list=train_and_val_subject_list, num_freq=loader.num_freq)\n",
    "                    data_train, label_train, data_val, label_val, train_subject_list = nLoader.getData(\n",
    "                        K_inner, in_fold)\n",
    "                    # print(in_fold,\" : \",label_train.shape,label_val.shape)\n",
    "                    curModel = copy.deepcopy(nModel)\n",
    "\n",
    "                    launch(curModel, data_train, label_train, data_val, label_val, device, optimizer, categories, dropout,\n",
    "                           batch_size, lr, l1_reg, l2_reg, mx_epoch,\n",
    "                           loader.num_freq, NodeDAT, EmotionDL, train_log)\n",
    "\n",
    "                    acc_list.append(\n",
    "                        (curModel.trainer.eval_acc_list, data_val.shape[0]))\n",
    "\n",
    "                    # calc the acc result under the given paras\n",
    "\n",
    "                acc_result = 0\n",
    "                tot_samples = 0\n",
    "                argmax_epoch = -1\n",
    "                acc_ep = []\n",
    "                for ep in grid_epoch:\n",
    "                    acc_ep.append(0)\n",
    "                    tot_samples = 0\n",
    "                    for fold in range(K_inner):\n",
    "                        acc_ep[-1] += acc_list[fold][0][ep] * \\\n",
    "                            acc_list[fold][1]\n",
    "                        tot_samples += acc_list[fold][1]\n",
    "                    # print(tot_samples,\" and \",data_train_and_val.shape[0])\n",
    "                    assert (tot_samples == data_train_and_val.shape[0])\n",
    "                    acc_ep[-1] /= tot_samples\n",
    "                acc_result = max(acc_ep)\n",
    "                argmax_epoch = np.argmax(np.array(acc_ep))+1\n",
    "\n",
    "                result_list.append(\n",
    "                    {\"paras\": paras, \"acc_mean\": acc_result, \"argmax_epoch\": argmax_epoch})\n",
    "            best_dict = result_list[0]\n",
    "            for i in range(1, len(result_list)):\n",
    "                if result_list[i][\"acc_mean\"] > best_dict[\"acc_mean\"]:\n",
    "                    best_dict = result_list[i]\n",
    "\n",
    "            nModel = copy.deepcopy(model)\n",
    "            paras = best_dict[\"paras\"]\n",
    "            fcv_epoch = best_dict[\"argmax_epoch\"]\n",
    "            if \"hiddens\" in paras:\n",
    "                nModel.num_hiddens = paras[\"hiddens\"]\n",
    "            if \"layers\" in paras:\n",
    "                nModel.num_layers = paras[\"layers\"]\n",
    "\n",
    "            lr, batch_size, dropout, l1_reg, l2_reg = getGridPara(paras)\n",
    "\n",
    "            launch(curModel, data_train_and_val, label_train_and_val, data_test, label_test, device, optimizer, categories, dropout,\n",
    "                   batch_size, lr, l1_reg, l2_reg, fcv_epoch,\n",
    "                   loader.num_freq, NodeDAT, EmotionDL, train_log)\n",
    "            out_acc_list.append(\n",
    "                {\"fold\": out_fold,\n",
    "                 \"best_paras\": best_dict[\"paras\"],\n",
    "                 \"train_acc_mean\": best_dict[\"acc_mean\"],\n",
    "                 \"test_acc_mean\": curModel.trainer.eval_acc_list[fcv_epoch-1],\n",
    "                 \"test_num_samples\": data_test.shape[0]})\n",
    "            # get best_tuple via inner_K fold cross-validation\n",
    "        mean_acc = 0\n",
    "        tot_samples = 0\n",
    "        for _ in out_acc_list:\n",
    "            mean_acc += _[\"test_acc_mean\"]*_[\"test_num_samples\"]\n",
    "            tot_samples += _[\"test_num_samples\"]\n",
    "        # print(tot_samples,\" && \",loader.data.shape[0])\n",
    "        assert (tot_samples == loader.data.shape[0])\n",
    "        mean_acc /= tot_samples\n",
    "        return mean_acc, out_acc_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "mamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
